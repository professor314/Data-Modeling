{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Lab Assignment 04 - Notebook.ipynb","provenance":[{"file_id":"1WEjv7P0HExeWrYIWMrYlAGVQdgnKox-G","timestamp":1622425056065}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JTFTwNGOtrEM"},"source":["# Lab Assignment 04 - Sentiment Analysis of Product Reviews Using Python\n","\n","In this lab assignment, we will be using a variety of tools and techniques to build two different kinds of sentiment analyzers. One of these will rely on a sentiment lexicon, while the other will rely on a machine learning model that will be trained using a complex feature vector representation of the input text objects.\n","\n","By the time you have completed this lab, you will have achieved all of the following learning objectives:\n","\n","## Learning Objectives\n","\n","* Load data from multiple spreadsheets in an Excel file into different Pandas dataframes.\n","* Compute sentiment polarity scores for text objects using a polarity lexicon.\n","* Assign predicted polarity labels using a quantile split of lexicon-based sentiment polarity scores.\n","* Compute TF-IDF vectors for a testing set using a vocabulary that was computed from a training set.\n","* Get part-of-speech (POS) tags for text objects.\n","* Calculate probability distributions for unigrams, bigrams, POS unigrams, and POS bigrams.\n","* Construct complex feature vector representations of text objects with varyind degrees of complexity.\n","* Evaluate and compare machine learning-based sentiment analysis models that have been trained using complex feature vectors.\n","* Continue to develop skills working with and analyzing text in Python."]},{"cell_type":"markdown","metadata":{"id":"Cc7VDHoEvQuP"},"source":["###Import Libraries\n","As usual, we will begin by importing all of the libraries that we'll need.\n","\n","Run the code cell below to import the libraries that we'll be using in this lab assignment."]},{"cell_type":"code","metadata":{"id":"-bBARytTBcQc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422150859,"user_tz":420,"elapsed":2427,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"c72d58c7-d0be-4bc7-c13f-578b06d175f3"},"source":["#import libraries\n","import nltk #the natural language toolkit\n","import numpy as np #used for vector / matrix operations\n","import pandas as pd\n","from collections import Counter #used to count occurrences of n-grams\n","from nltk import pos_tag #used to generate part-of-speech (POS) tags\n","from nltk.tokenize import word_tokenize #used to split text into tokens\n","from sklearn.feature_extraction.text import TfidfVectorizer #used to generate TF-IDF vectors and build the vocabulary\n","from sklearn.linear_model import LogisticRegression #used to train logistic regression-based classifiers\n","from sklearn.model_selection import train_test_split #used to split the data into training and testing sets\n","nltk.download('averaged_perceptron_tagger') #needed to generate POS tags\n","nltk.download('punkt') #needed to tokenize text"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"3CcKpJZv97Fi"},"source":["### Load Data\n","The data for this lab assignment are contained in two different spreadsheets inside an Excel file. The first of these spreadsheets contains data for a large number of product reviews, with each record involving a textual review and a sentiment polarity label. The meanings of these polarity labels are:\n","* 1 = positive\n","* 0 = neutral\n","* -1 = negative\n","\n","The second spreadsheet contains a sentiment polarity lexicon that was constructed on the basis of Twitter hashtags. Each record in the lexicon consists of an n-gram (either a unigram or a bigram) along with a corresponding sentiment polarity score for that n-gram. The polarity scores in the lexicon range from -1.0 (very negative) to +1.0 (very positive).\n","\n","Run the code cell below to load the two spreadsheets into your Python program. Note that each spreadsheet is loaded into its own Pandas dataframe."]},{"cell_type":"code","metadata":{"id":"rhiTukoLQQet","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1622422199268,"user_tz":420,"elapsed":22101,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"0a515a9a-9099-47ad-f042-7b55cd9f29c4"},"source":["#load data into dataframes\n","df = pd.read_excel('Lab Assignment 04 - Data.xlsx', sheet_name=0)\n","df_lexicon = pd.read_excel('Lab Assignment 04 - Data.xlsx', sheet_name=1)\n","\n","#display the first few rows of product review data\n","df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_text</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Really pathetic: I bought this at the airport ...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Am I missing stomething?: The characters are n...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great--as a Comedy! Where's the Zero Stars but...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>WRONG PART SENT...: HI! I THINK THAT I GOT WRO...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Try to find a replacement gasket: Spend the mo...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>valeo ball: When i received this item not only...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Cuisipro Rotary Cheese Grater: This tool is ve...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>on behalf of all christians out there...i'm so...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>BAD CUSTOMER SERVICE: My clock ..was defective...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Very disappointed: After purchasing and spendi...</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         review_text  polarity\n","0  Really pathetic: I bought this at the airport ...        -1\n","1  Am I missing stomething?: The characters are n...        -1\n","2  Great--as a Comedy! Where's the Zero Stars but...        -1\n","3  WRONG PART SENT...: HI! I THINK THAT I GOT WRO...        -1\n","4  Try to find a replacement gasket: Spend the mo...        -1\n","5  valeo ball: When i received this item not only...        -1\n","6  Cuisipro Rotary Cheese Grater: This tool is ve...        -1\n","7  on behalf of all christians out there...i'm so...        -1\n","8  BAD CUSTOMER SERVICE: My clock ..was defective...        -1\n","9  Very disappointed: After purchasing and spendi...        -1"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"LiLlPkTECwNc"},"source":["Run the code cell below to display the last 10 rows of data in the sentiment polarity lexicon."]},{"cell_type":"code","metadata":{"id":"j-HolKDrDEpe","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1622422212875,"user_tz":420,"elapsed":138,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"df229a91-f849-4e76-cf98-bd55023c3778"},"source":["#display the last 10 rows of data in the polarity lexicon\n","df_lexicon.tail(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ngram</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>354915</th>\n","      <td>zumba today</td>\n","      <td>0.040338</td>\n","    </tr>\n","    <tr>\n","      <th>354916</th>\n","      <td>zurich</td>\n","      <td>-0.005991</td>\n","    </tr>\n","    <tr>\n","      <th>354917</th>\n","      <td>zzz</td>\n","      <td>-0.009528</td>\n","    </tr>\n","    <tr>\n","      <th>354918</th>\n","      <td>zzzquil</td>\n","      <td>0.165744</td>\n","    </tr>\n","    <tr>\n","      <th>354919</th>\n","      <td>zzzz</td>\n","      <td>-0.061790</td>\n","    </tr>\n","    <tr>\n","      <th>354920</th>\n","      <td>zzzz !</td>\n","      <td>0.040338</td>\n","    </tr>\n","    <tr>\n","      <th>354921</th>\n","      <td>zzzzz</td>\n","      <td>0.096251</td>\n","    </tr>\n","    <tr>\n","      <th>354922</th>\n","      <td>zzzzz !</td>\n","      <td>0.040338</td>\n","    </tr>\n","    <tr>\n","      <th>354923</th>\n","      <td>zzzzzz</td>\n","      <td>-0.084955</td>\n","    </tr>\n","    <tr>\n","      <th>354924</th>\n","      <td>zzzzzzz</td>\n","      <td>-0.038626</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              ngram  polarity\n","354915  zumba today  0.040338\n","354916       zurich -0.005991\n","354917          zzz -0.009528\n","354918      zzzquil  0.165744\n","354919         zzzz -0.061790\n","354920       zzzz !  0.040338\n","354921        zzzzz  0.096251\n","354922      zzzzz !  0.040338\n","354923       zzzzzz -0.084955\n","354924      zzzzzzz -0.038626"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"6DB_StrlDuoA"},"source":["###Build an N-Gram Polarity Dictionary\n","The polarity lexicon contains a large number of n-grams and polarity scores. Since we will need to perform a lot of searches for n-grams and their polarities in this lab assignment, it will be convenient (and much faster!) to work with these data in the form of a dictionary.\n","\n","Run the code cell below to build an n-gram polarity dictionary whose keys are n-grams and whose values are the n-grams' corresponding polarity scores."]},{"cell_type":"code","metadata":{"id":"6BewENQ6OmT9"},"source":["#build the n-gram polarity dictionary\n","ngram_polarity = {}\n","for row in df_lexicon.itertuples():\n","  ngram = str(row.ngram)\n","  ngram_polarity[ngram] = row.polarity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvaNNAtaLlQs"},"source":["**TASK 01:**\n",">Write a line of code in the cell below that will display the total number of n-grams in the n-gram polarity dictionary.\n","\n","**QUESTION 01:**\n",">How many n-grams are in the n-gram polarity dictionary?"]},{"cell_type":"code","metadata":{"id":"lHn5-zbpEJ7t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422302066,"user_tz":420,"elapsed":133,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"2b98eb1b-7e22-463f-e1fb-0f42ba946ed2"},"source":["#display the total number of n-grams in the n-gram polarity dictionary\n","len(ngram_polarity)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["354925"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"oGReFRf7FkL1"},"source":["###Examining N-Gram Polarities\n","As noted previously, our n-gram polarity lexicon was constructed on the basis of Twitter hashtags. Let's examine the polarity scores for a few n-grams to see if this approach to constructing a polarity lexicon is viable.\n","\n","Run the code cell below to display the polarity scores for a list of n-grams. Remember that the polarity scores in the dictionary range from -1.0 (very negative) to + 1.0 (very positive)."]},{"cell_type":"code","metadata":{"id":"GGJ1e3qeZ9uf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422333899,"user_tz":420,"elapsed":156,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"eecc0de7-e1c8-4fea-9cda-45e5c988c398"},"source":["#display sentiment polarity scores for a list of n-grams\n","ngrams = ['very good', 'good', 'pretty good', 'not good']\n","for ngram in ngrams:\n","  print(ngram, ngram_polarity[ngram])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["very good 0.9099999999999999\n","good 0.7\n","pretty good 0.475\n","not good -0.35\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pqNXg5VCL0Gv"},"source":["**TASK 02:**\n",">Write some code in the cell below that will display sentiment polarity scores for the n-grams 'not bad', 'pretty bad', 'bad', and 'very bad'.\n","\n","**QUESTION 02:**\n",">Which of the following statements about the n-gram polarities are true?\n","* 'not bad' has a negative polarity score\n","* 'pretty bad' is more negative than 'bad'\n","* 'bad' is more negative than 'pretty bad'\n","* both 'not bad' and 'pretty bad' have positive polarity scores"]},{"cell_type":"code","metadata":{"id":"TtaURHernkGd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422437765,"user_tz":420,"elapsed":133,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"135d8a46-1e39-4b60-a2e5-be6ec53d00e2"},"source":["#display sentiment polarity scores for the n-grams 'not bad', 'pretty bad', 'bad', and 'very bad'\n","ngrams = ['not bad', 'pretty bad', 'bad', 'very bad']\n","for ngram in ngrams:\n","  print(ngram, ngram_polarity[ngram])\n","\n","#'bad' is more negative than 'pretty bad'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["not bad 0.3499999999999999\n","pretty bad -0.2249999999999999\n","bad -0.6999999999999998\n","very bad -0.9099999999999998\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I2ZTfVYtIRNO"},"source":["###Computing Sentiment Polarity Scores Using a Polarity Lexicon\n","The general idea behind using a polarity lexicon to compute sentiment polarity scores for input text is quite simple: The overall polarity score for the input text is simply the average of the polarity scores for each n-gram that appears in the text. To assign a polarity score to any input text, we simply need to identify the n-grams that appear in the text, look up their corresponding polarity scores in the polarity lexicon, and then compute the average of those scores.\n","\n","Run the code cell below to add a `get_lexicon_polarity()` function to your Python program. This function uses the lexicon-based n-gram polarities to compute an average polarity score for the input text. Since the polarities in the lexicon are all in the range -1.0 to +1.0, the average polarity score will also always be between -1.0 and +1.0."]},{"cell_type":"code","metadata":{"id":"D07I9ZByejfW"},"source":["#define a function that will use the lexicon-based n-gram polarities to compute a \n","#polarity score between -1.0 and +1.0 for the input text.\n","def get_lexicon_polarity(raw_text):\n","  #define a variable to hold a running total of polarity scores\n","  polarity = 0.0\n","  #define a variable to hold the total number of times n-grams in the polarity\n","  #dictionary appeared in the input text\n","  total_ngram_matches = 0\n","  #convert the input text to lowercase (since all of the n-grams in the polarity\n","  #dictionary are in lowercase)\n","  text = raw_text.lower()\n","  #tokenize the input text\n","  tokens = word_tokenize(text)\n","  #construct a list containing all of the bigrams in the input text\n","  bigrams = []\n","  for i in range(1, len(tokens)):\n","    bigrams.append(tokens[i - 1] + ' ' + tokens[i]) #build all bigrams\n","  #compute running polarity sum and number of n-gram matches\n","  for bigram in bigrams:\n","    #if this bigram appears in the polarity dictionary\n","    if bigram in ngram_polarity:\n","      polarity += ngram_polarity[bigram] #update running total\n","      total_ngram_matches += 1 #update number of n-gram matches\n","    else: #if this bigram does not appear in the polarity dictionary\n","      left_unigram = bigram.split()[0] #get the unigram on the left side of the bigram\n","      #if this unigram appears in the polarity dictionary\n","      if left_unigram in ngram_polarity:\n","        polarity += ngram_polarity[left_unigram] #update running total\n","        total_ngram_matches += 1 #update number of n-gram matches\n","  #compute the overall average polarity score for the input text\n","  polarity /= total_ngram_matches\n","  #return the polarity score\n","  return polarity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1vKUiWTL3bu"},"source":["**TASK 03:**\n",">Write a line of code in the cell below that will display the lexicon-based sentiment polarity score for the following sentence: *'Hiking in the mountains is fun and very relaxing.'*\n","\n","**QUESTION 03:**\n",">What is the lexicon-based sentiment polarity score for the sentence *'Hiking in the mountains is fun and very relaxing.'* ? Report your answer using three decimals of precision (e.g., 0.321)."]},{"cell_type":"code","metadata":{"id":"v97YAuSumKo8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422549970,"user_tz":420,"elapsed":191,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"e7fa6440-e02e-42a4-a405-910a85ec7080"},"source":["#display the lexicon-based sentiment polarity score for the \n","#sentence 'Hiking in the mountains is fun and very relaxing.'\n","get_lexicon_polarity('Hiking in the mountains is fun and very relaxing.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.22660910468293366"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"lavTeunSLOjF"},"source":["Congratulations, you've just built a complete sentiment analyzer!"]},{"cell_type":"markdown","metadata":{"id":"xu_cKzq-MqbM"},"source":["###Compute Lexicon-Based Polarity Scores for Each Product Review\n","Next, let's use our `get_lexicon_polarity()` function to compute lexicon-based polarity scores for each product review in the dataframe.\n","\n","Run the code cell below to compute a polarity score for each product review and add those polarity scores to the dataframe."]},{"cell_type":"code","metadata":{"id":"komI28L1pvB-","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1622422572481,"user_tz":420,"elapsed":2237,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"23ecb0e7-d0a2-49b9-b13b-bd3a12b66747"},"source":["#compute lexicon-based polarity scores for each review\n","lexicon_polarities = []\n","for text in df.review_text:\n","  lexicon_polarities.append(get_lexicon_polarity(text))\n","\n","#add lexicon-based polarity scores to the dataframe\n","df['lexicon_polarity'] = lexicon_polarities\n","\n","#display the first 10 rows in the dataframe\n","df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_text</th>\n","      <th>polarity</th>\n","      <th>lexicon_polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Really pathetic: I bought this at the airport ...</td>\n","      <td>-1</td>\n","      <td>-0.035038</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Am I missing stomething?: The characters are n...</td>\n","      <td>-1</td>\n","      <td>-0.063212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great--as a Comedy! Where's the Zero Stars but...</td>\n","      <td>-1</td>\n","      <td>-0.016061</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>WRONG PART SENT...: HI! I THINK THAT I GOT WRO...</td>\n","      <td>-1</td>\n","      <td>-0.034859</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Try to find a replacement gasket: Spend the mo...</td>\n","      <td>-1</td>\n","      <td>-0.060013</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>valeo ball: When i received this item not only...</td>\n","      <td>-1</td>\n","      <td>-0.044593</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Cuisipro Rotary Cheese Grater: This tool is ve...</td>\n","      <td>-1</td>\n","      <td>-0.037988</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>on behalf of all christians out there...i'm so...</td>\n","      <td>-1</td>\n","      <td>-0.054484</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>BAD CUSTOMER SERVICE: My clock ..was defective...</td>\n","      <td>-1</td>\n","      <td>-0.070526</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Very disappointed: After purchasing and spendi...</td>\n","      <td>-1</td>\n","      <td>-0.054537</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         review_text  ...  lexicon_polarity\n","0  Really pathetic: I bought this at the airport ...  ...         -0.035038\n","1  Am I missing stomething?: The characters are n...  ...         -0.063212\n","2  Great--as a Comedy! Where's the Zero Stars but...  ...         -0.016061\n","3  WRONG PART SENT...: HI! I THINK THAT I GOT WRO...  ...         -0.034859\n","4  Try to find a replacement gasket: Spend the mo...  ...         -0.060013\n","5  valeo ball: When i received this item not only...  ...         -0.044593\n","6  Cuisipro Rotary Cheese Grater: This tool is ve...  ...         -0.037988\n","7  on behalf of all christians out there...i'm so...  ...         -0.054484\n","8  BAD CUSTOMER SERVICE: My clock ..was defective...  ...         -0.070526\n","9  Very disappointed: After purchasing and spendi...  ...         -0.054537\n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"9-ZoVyxETQYi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422587157,"user_tz":420,"elapsed":139,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"2489d587-70e4-4cd2-c9f8-531c4d56a76c"},"source":["#display descriptive statistices for our new lexicon-based polarity scores\n","df.lexicon_polarity.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    3000.000000\n","mean        0.020286\n","std         0.072711\n","min        -0.349723\n","25%        -0.033063\n","50%         0.017058\n","75%         0.065085\n","max         0.391625\n","Name: lexicon_polarity, dtype: float64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"icwt_7YrPoBy"},"source":["###Predict the Sentiment Polarity Label for Each Product Review"]},{"cell_type":"code","metadata":{"id":"SjxjfuaOTmC4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622422592045,"user_tz":420,"elapsed":126,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"a06caff9-d439-41bb-e3c0-39ed7a9a6303"},"source":["#compute tertiles (i.e., quantiles for a three-way split) for the lexicon polarity scores\n","tertiles = df.lexicon_polarity.quantile([1/3, 2/3])\n","tertiles"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.333333   -0.014254\n","0.666667    0.043551\n","Name: lexicon_polarity, dtype: float64"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"va6PXVbSUZcJ","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1622422597870,"user_tz":420,"elapsed":130,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"50310c2d-32e1-47fd-e6f9-051052fda404"},"source":["#compute predicted polarity labels using quantiles for lexicon-based polarity scores\n","lexicon_polarities = []\n","for lexicon_polarity in df.lexicon_polarity:\n","  if lexicon_polarity <= -0.014732: #if this lexicon polarity score is in the first tertile\n","    lexicon_polarities.append(-1) #assign a label of \"-1\"\n","  elif lexicon_polarity <= 0.042543: #if this lexicon polarity score is in the second tertile\n","    lexicon_polarities.append(0) #assign a label of \"0\"\n","  else: #if this lexicon polarity score is in the third tertile\n","    lexicon_polarities.append(1) #assign a label of \"1\"\n","\n","#store lexicon-based predicted polarity labels in the dataframe\n","df['lexicon_polarity'] = lexicon_polarities\n","\n","#display the first 10 rows in the dataframe\n","df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_text</th>\n","      <th>polarity</th>\n","      <th>lexicon_polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Really pathetic: I bought this at the airport ...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Am I missing stomething?: The characters are n...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great--as a Comedy! Where's the Zero Stars but...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>WRONG PART SENT...: HI! I THINK THAT I GOT WRO...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Try to find a replacement gasket: Spend the mo...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>valeo ball: When i received this item not only...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Cuisipro Rotary Cheese Grater: This tool is ve...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>on behalf of all christians out there...i'm so...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>BAD CUSTOMER SERVICE: My clock ..was defective...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Very disappointed: After purchasing and spendi...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         review_text  ...  lexicon_polarity\n","0  Really pathetic: I bought this at the airport ...  ...                -1\n","1  Am I missing stomething?: The characters are n...  ...                -1\n","2  Great--as a Comedy! Where's the Zero Stars but...  ...                -1\n","3  WRONG PART SENT...: HI! I THINK THAT I GOT WRO...  ...                -1\n","4  Try to find a replacement gasket: Spend the mo...  ...                -1\n","5  valeo ball: When i received this item not only...  ...                -1\n","6  Cuisipro Rotary Cheese Grater: This tool is ve...  ...                -1\n","7  on behalf of all christians out there...i'm so...  ...                -1\n","8  BAD CUSTOMER SERVICE: My clock ..was defective...  ...                -1\n","9  Very disappointed: After purchasing and spendi...  ...                -1\n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"0rYgvU-hL9XL"},"source":["**TASK 04:**\n",">Write a line of code in the cell below that will display the number of positive product reviews in the dataframe that were correctly labeled as positive by the lexicon-based sentiment analyzer. ***Hint:*** the Pandas `crosstab()` function may prove useful!\n","\n","**QUESTION 04:**\n",">How many positive product reviews in the dataframe  were correctly labeled as positive by the lexicon-based sentiment analyzer?"]},{"cell_type":"code","metadata":{"id":"h3BaDYwwWko5","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1622423097934,"user_tz":420,"elapsed":140,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"9538b104-8c18-497c-c060-a83e80aa73d9"},"source":["#display the number of positive product reviews that were correctly labeled as positive by\n","#the lexicon-based sentiment analyzer\n","lexicon_polarities\n","pd.crosstab(df.polarity,df.lexicon_polarity, dropna=False)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>lexicon_polarity</th>\n","      <th>-1</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","    <tr>\n","      <th>polarity</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>-1</th>\n","      <td>925</td>\n","      <td>72</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>66</td>\n","      <td>800</td>\n","      <td>134</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>125</td>\n","      <td>874</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["lexicon_polarity   -1    0    1\n","polarity                       \n","-1                925   72    3\n"," 0                 66  800  134\n"," 1                  1  125  874"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"8AEcSa0oMAbJ"},"source":["**TASK 05:**\n",">Write some code in the cell below that will determine the overall accuracy of the polarity label predictions that were made by the lexicon-based sentiment analyzer.\n","\n","**QUESTION 05:**\n",">What is the overall accuracy of the polarity label predictions that were made by the lexicon-based sentiment analyzer? Report your answer using three decimals of precision (e.g., 0.876)"]},{"cell_type":"code","metadata":{"id":"geekvopvXCT0"},"source":["#determine the overall accuracy of the lexicon-based sentiment analyzer's polarity label predictions\n","#2599/3000 = .866"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imDeLEz-QIgY"},"source":["##Part 02 - Machine Learning-Based Sentiment Polarity Classification"]},{"cell_type":"markdown","metadata":{"id":"I4GJxUsdQWrA"},"source":["###Split Data into Training and Testing Sets\n","Since we'll be working with supervised machine learning models from this point forward, we'll need to split our data into training and testing sets. Our models will be trained using the training data and evaluated using the testing data. In this way, we'll have a good understanding of how a model could be expected to perform in the real world.\n","\n","Run the code cell below to split the data into two dataframes, one of which contains the training data and the other of which contains the testing data."]},{"cell_type":"code","metadata":{"id":"uopy5lBuRgxw"},"source":["#split data into training and testing sets\n","df_train, df_test = train_test_split(df.copy(), test_size=0.3, shuffle=True, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4e5GjdsHMDYF"},"source":["**TASK 06:**\n",">Write a line of code in the cell below that will display the number of rows in the training dataframe.\n","\n","**QUESTION 06:**\n",">How many rows are in the training dataframe?"]},{"cell_type":"code","metadata":{"id":"qnLdy12FeG-_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622423367252,"user_tz":420,"elapsed":162,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"19b8fe64-7896-40d7-d55d-d2d5956fbbf4"},"source":["#display the number of rows in the training dataframe\n","df_train.shape\n","#2100 rows"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2100, 3)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"QIq-sOvSQ2bi"},"source":["###Compute TF-IDF Scores for the Product Reviews\n","Next, let's compute the TF-IDF scores for each review in the training and testing sets. Note that the TF-IDF scores for the reviews in the *testing* set are computed using the vocabulary from the *training* set. Again, this is necessary to reflect real-world conditions in which new reviews for which we are assigning polarity labels would not have been used to construct the vocabulary.\n","\n","Run the code cell below to compute the TF-IDF scores for the reviews in the training and testing sets."]},{"cell_type":"code","metadata":{"id":"VQqNNtUkeRv-"},"source":["#build the vocabulary of unique words and compute TF-IDF scores for each review in the training set\n","vectorizer = TfidfVectorizer()\n","train_tfidf_scores = np.array(vectorizer.fit_transform(df_train.review_text).todense())\n","vocabulary = vectorizer.vocabulary_\n","\n","#compute TF-IDF scores for each review in the testing set using the vocabulary from the training set\n","test_tfidf_scores = np.array(vectorizer.transform(df_test.review_text).todense())\n","\n","#add TF-IDF scores to the training and testing dataframes\n","df_train['tfidf_scores'] = [tfidf_scores for tfidf_scores in train_tfidf_scores]\n","df_test['tfidf_scores'] = [tfidf_scores for tfidf_scores in test_tfidf_scores]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5T_iTyRxRK4X"},"source":["###Identify All of the Unique Unigrams, Bigrams, POS Unigrams, and POS Bigrams in the Training Data\n","Next, we need to identify the set of unique text unigrams, text bigrams, POS unigrams, and POS bigrams that appear in the training data. These lists will serve as the basis for calculating the corresponding probability distributions for each review.\n","\n","Run the code cell below to generate lists of all of the unique unigrams, bigrams, POS unigrams, and POS bigrams that appear in the training data."]},{"cell_type":"code","metadata":{"id":"oZGVda-XhsAW"},"source":["#get the combined text for all of the reviews in the training set\n","all_text = ' '.join(df_train.review_text)\n","\n","#tokenize the text\n","tokens = word_tokenize(all_text)\n","\n","#compute unigrams and bigrams for the text\n","unigrams = list(nltk.ngrams(tokens, n=1))\n","bigrams = list(nltk.ngrams(tokens, n=2, pad_left=True, pad_right=True, \n","                      left_pad_symbol='<s>', right_pad_symbol='</s>'))\n","\n","#generate part-of-speech (POS) tags for the tokens\n","pos_tags = pos_tag(tokens)\n","\n","#extract just the POS tags from the POS tuples\n","pos_tags = [pos_tag for token, pos_tag in pos_tags]\n","\n","#compute unigrams and bigrams for the POS tags\n","pos_unigrams = list(nltk.ngrams(pos_tags, n=1))\n","pos_bigrams = list(nltk.ngrams(pos_tags, n=2, pad_left=True, pad_right=True, \n","                      left_pad_symbol='<s>', right_pad_symbol='</s>'))\n","\n","#get lists of unique unigrams, bigrams, POS unigrams, and POS bigrams from the training data\n","unigrams = list(Counter(unigrams).keys())\n","bigrams = list(Counter(bigrams).keys())\n","pos_unigrams = list(Counter(pos_unigrams).keys())\n","pos_bigrams = list(Counter(pos_bigrams).keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52Iyoc3XMHtO"},"source":["**TASK 07:**\n",">Write some code in the cell below that will display the total number of unique unigrams, bigrams, POS unigrams, POS bigrams, and vocabulary words in the training set. Also compute and display the sum of all of these values, which will reveal the total number of available features.\n","\n","**QUESTION 07:**\n",">What is the total number of features among the unigrams, bigrams, POS unigrams, POS bigrams, and vocabulary words?"]},{"cell_type":"code","metadata":{"id":"NwTRMZIKkVwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424095236,"user_tz":420,"elapsed":133,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"326d3beb-0472-4597-d7d8-398a56bd510f"},"source":["#display the total number of unique unigrams, bigrams, POS unigrams, POS bigrams, and vocabulary words \n","#in the training set, as well as the sum of all of these values\n","len(unigrams)+len(bigrams)+len(pos_unigrams)+len(pos_bigrams)+len(vocabulary)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["119765"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"Kr7bxjtgSwno"},"source":["###Compute Unigram, Bigram, POS Unigram, and POS Bigram Probability Distributions\n","Next, we'll create a function that will be able to compute unigram, bigram, POS unigram, and POS bigram probability distributions for any input text. These probability distributions will be based on the lists of unique unigrams, bigrams, POS unigrams, and POS bigrams that were identified previously.\n","\n","Run the code cell below to add a `get_probability_distributions()` function to your Python program. The code for this function may appear to be long and complicated, but it's actually quite simple. Since we need to compute four different probability distributions, most of the code is just repeated four times."]},{"cell_type":"code","metadata":{"id":"kbH2iEbFW3HP"},"source":["#define a function that will compute unigram, bigram, POS unigram, and POS bigram probability distributions \n","#for the specified review text\n","def get_probability_distributions(review_text):\n","  #tokenize the text\n","  review_tokens = word_tokenize(review_text)\n","  #compute unigrams and bigrams for the text\n","  review_unigrams = list(nltk.ngrams(review_tokens, n=1))\n","  review_bigrams = list(nltk.ngrams(review_tokens, n=2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n","  #generate part-of-speech (POS) tags for the tokens\n","  review_pos_tags = pos_tag(review_tokens)\n","  #extract just the POS tags from the POS tuples\n","  review_pos_tags = [pos_tag for token, pos_tag in review_pos_tags]\n","  #compute unigrams and bigrams for the POS tags\n","  review_pos_unigrams = list(nltk.ngrams(review_pos_tags, n=1))\n","  review_pos_bigrams = list(nltk.ngrams(review_pos_tags, n=2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n","  #compute unigram, bigram, POS unigram, and POS bigram frequency distributions for this review\n","  review_unigram_frequencies = Counter(review_unigrams)\n","  review_bigram_frequencies = Counter(review_bigrams)\n","  review_pos_unigram_frequencies = Counter(review_pos_unigrams)\n","  review_pos_bigram_frequencies = Counter(review_pos_bigrams)\n","  #get total number of ngram occurrences for each frequency distribution\n","  n_review_unigram_frequencies = sum(review_unigram_frequencies.values())\n","  n_review_bigram_frequencies = sum(review_bigram_frequencies.values())\n","  n_review_pos_unigram_frequencies = sum(review_pos_unigram_frequencies.values())\n","  n_review_pos_bigram_frequencies = sum(review_pos_bigram_frequencies.values())\n","  #compute unigram probability distribution\n","  unigram_probabilities = np.zeros(len(unigrams))\n","  for i in range(len(unigrams)):\n","    if unigrams[i] in review_unigram_frequencies:\n","      unigram_probabilities[i] = review_unigram_frequencies[unigrams[i]] / n_review_unigram_frequencies\n","  #compute bigram probability distribution\n","  bigram_probabilities = np.zeros(len(bigrams))\n","  for i in range(len(bigrams)):\n","    if bigrams[i] in review_bigram_frequencies:\n","      bigram_probabilities[i] = review_bigram_frequencies[bigrams[i]] / n_review_bigram_frequencies\n","  #compute POS unigram probability distribution\n","  pos_unigram_probabilities = np.zeros(len(pos_unigrams))\n","  for i in range(len(pos_unigrams)):\n","    if pos_unigrams[i] in review_pos_unigram_frequencies:\n","      pos_unigram_probabilities[i] = review_pos_unigram_frequencies[pos_unigrams[i]] / n_review_pos_unigram_frequencies\n","  #compute POS bigram probability distribution\n","  pos_bigram_probabilities = np.zeros(len(pos_bigrams))\n","  for i in range(len(pos_bigrams)):\n","    if pos_bigrams[i] in review_pos_bigram_frequencies:\n","      pos_bigram_probabilities[i] = review_pos_bigram_frequencies[pos_bigrams[i]] / n_review_pos_bigram_frequencies\n","  #return the probability distributions\n","  return unigram_probabilities, bigram_probabilities, pos_unigram_probabilities, pos_bigram_probabilities"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XSROYzHsR04"},"source":["Now we're ready to actually compute the unigram, bigram, POS unigram, and POS bigram probability distributions. Yay!\n","\n","Run the code cell below to compute and add all of the probability distributions for the training and testing data to their respective dataframes.\n","\n","***Note:*** This will take about 90 seconds to run."]},{"cell_type":"code","metadata":{"id":"SFaGvbJPat8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424195484,"user_tz":420,"elapsed":79757,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"4ffe1986-5d3c-47b9-e966-de0308108d44"},"source":["#compute and add the unigram, bigram, POS unigram, and POS bigram probability distributions for each training review to the dataframe\n","df_train[['unigram_probs', 'bigram_probs', 'pos_unigram_probs', 'pos_bigram_probs']] = [get_probability_distributions(review_text) for review_text in df_train.review_text]\n","\n","#compute and add the unigram, bigram, POS unigram, and POS bigram probability distributions for each testing review to the dataframe\n","df_test[['unigram_probs', 'bigram_probs', 'pos_unigram_probs', 'pos_bigram_probs']] = [get_probability_distributions(review_text) for review_text in df_test.review_text]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MRUHcYXnTZDr"},"source":["###Train and Test Machine Learning-Based Sentiment Polarity Classifiers\n","We now have all of the features that we'll need to train a machine learning-based sentiment polarity classifier.\n","\n","For the rest of this lab assignment, we'll be training and evaluating the polarity classification performance of machine learning models that have been trained using different feature vectors. We'll begin by testing a model that is trained using just the POS n-gram information, after which we'll test a model that is trained using both the text n-gram and POS n-gram probability distributions. The final, most complex model will add the TF-IDF scores to the feature vector representation.\n","\n","Run the code cell below to build training and testing feature vectors that are composed of just the POS unigram probabilities and the POS bigram probabilities."]},{"cell_type":"code","metadata":{"id":"3FEIh3dIojS4"},"source":["#build training and testing feature vectors that contain just the POS unigrams and bigrams\n","training_features = []\n","#for each row in the training dataframe\n","for row in df_train.itertuples():\n","  #combine the two feature vectors\n","  feature_vector = np.append(row.pos_unigram_probs, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  training_features.append(feature_vector)\n","\n","testing_features = []\n","#for each row in the testing dataframe\n","for row in df_test.itertuples():\n","  #combine the two feature vectors\n","  feature_vector = np.append(row.pos_unigram_probs, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  testing_features.append(feature_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HqQiX7u2h_NK"},"source":["Now that we have our first set of feature vectors, we need to get their corresponding labels.\n","\n","Run the code cell below to get the polarity labels for the training and testing datasets. Note that we will be able to reuse these labels for each of the models that we train -- although the feature vectors will change from model to model, the labels are always the same."]},{"cell_type":"code","metadata":{"id":"vYuYDizL2Ayv"},"source":["#get the training and testing labels (polarity labels)\n","training_labels = df_train.polarity.to_list()\n","testing_labels = df_test.polarity.to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"duE7JyE3itGS"},"source":["We now have our labels and our first set of feature vectors, so we're finally ready to train a sentiment polarity classfier. We'll be training ordinal logistic regression classifiers in this lab assignment, but we could easily try other types of classifiers, as well.\n","\n","Run the code cell below to define a logisitic regression classifier and train that classifier using the POS unigram and POS bigram feature vectors. After the model is trained, the overall accuracy of its predictions on the **training** set will be displayed. Remember, the model is attempting to classify each review as having a positive, neutral, or negative polarity label."]},{"cell_type":"code","metadata":{"id":"MK9k3jJCEhNv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424636435,"user_tz":420,"elapsed":41568,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"5f6af7c5-693a-4b6a-f1e4-abe4f3695900"},"source":["#define a logistic regression classifier\n","model = LogisticRegression(random_state=42)\n","\n","#train the logistic regression classifier using the training data\n","model.fit(training_features, training_labels)\n","\n","#calculate and display the training accuracy\n","training_accuracy = model.score(training_features, training_labels)\n","print('Training accuracy: {:.3f}'.format(training_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training accuracy: 0.650\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ng-ODKfIMRd6"},"source":["**TASK 08:**\n",">Write some code in the cell below that will display the **testing** accuracy for the logistic regression classifer (i.e., the performance of the classifier on the testing data).\n","\n","**QUESTION 08:**\n",">What is the testing accuracy for the logistic regression classifer? Report your answer using three decimals of precision (e.g., 0.765)."]},{"cell_type":"code","metadata":{"id":"VDhda1ed3Jo1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424641392,"user_tz":420,"elapsed":400,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"93550201-be21-47e2-9622-d17da3da21c5"},"source":["#calculate and display the testing accuracy for the logistic regression classifier\n","#model.fit(testing_features, testing_labels)\n","testing_accuracy = model.score(testing_features, testing_labels)\n","print('Testing accuracy: {:.3f}'.format(testing_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing accuracy: 0.617\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UNblJBeSkOd-"},"source":["Recall that our dataset contains an equal number of positive, neutral, and negative reviews. This means that our baseline for judging the classification accuracy of our models is 0.3333 (or 33.33%), since this is the level of classification accuracy that we would expect by random guessing."]},{"cell_type":"markdown","metadata":{"id":"GzpS4A_LUeqt"},"source":["####Add Text Unigrams and Bigrams to the Feature Vector Representation\n","Next, let's build some more complex feature vectors that will contain additional information about the source text. Specifically, instead of using just the POS n-gram probabilities, we'll build feature vectors that are comprised of the text unigram and bigram probabilities, as well as the POS unigram and bigram probabilities.\n","\n","Run the code cell below to build these more complex feature vectors."]},{"cell_type":"code","metadata":{"id":"EHnd3GvupCgn"},"source":["#build training and testing feature vectors that contain the unigrams, bigrams, POS unigrams, and POS bigrams\n","training_features = []\n","#for each row in the training dataframe\n","for row in df_train.itertuples():\n","  #combine the feature vectors\n","  feature_vector = np.append(row.unigram_probs, row.bigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_unigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  training_features.append(feature_vector)\n","\n","testing_features = []\n","#for each row in the testing dataframe\n","for row in df_test.itertuples():\n","  #combine the feature vectors\n","  feature_vector = np.append(row.unigram_probs, row.bigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_unigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  testing_features.append(feature_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPjz8tY36acI"},"source":["Now that we have our more complex feature vectors, let's use those feature vectors to train a new logisitic regression classifier.\n","\n","Run the code cell below to train a logistic regression classifier using the more complex training feature vectors.\n","\n","***Note:*** This will take about 60 seconds to run."]},{"cell_type":"code","metadata":{"id":"Edh-hBgsMV6u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424555841,"user_tz":420,"elapsed":40547,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"63ae37f5-cf99-43e1-d153-d41573769b4c"},"source":["#train the logistic regression classifier using the more complex training feature vectors\n","model.fit(training_features, training_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"hRZBYNRK5vqf"},"source":["**TASK 09:**\n",">Write some code in the cell below that will calculate and display the testing accuracy after training the model using the more complex feature vector representation (i.e., after adding the text unigrams and bigrams to the feature vectors).\n","\n","**QUESTION 09:**\n",">What is the testing accuracy for the model that was trained using the more complex feature vector representation (i.e., the feature vector representation that includes the text unigrams and bigrams)? Report your answer using three decimals of precision (e.g., 0.783)."]},{"cell_type":"code","metadata":{"id":"6uMEIkHs6L1H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622424564212,"user_tz":420,"elapsed":471,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"671b61ce-6ddf-4792-f6ae-5dc38370583a"},"source":["#calculate and display the testing accuracy for the more complex feature vector representation\n","#model.fit(testing_features, testing_labels)\n","testing_accuracy = model.score(testing_features, testing_labels)\n","print('Testing accuracy: {:.3f}'.format(testing_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing accuracy: 0.617\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9WN_GLIlWjgv"},"source":["####Add TF-IDF Scores to the Feature Vector Representation\n","Finally, we'll build the most complex feature vector representation of the source text. Specifically, these feature vectors will consist of the text unigram and bigram probabilities, the POS unigram and bigram probabilities, and the TF-IDF scores for each review."]},{"cell_type":"markdown","metadata":{"id":"VVVGy9vP6szL"},"source":["**TASK 10:**\n",">Write some code in the cells below that will:\n","1. Add the TF-IDF scores to the feature vector representation;\n","2. Train the logistic regression classifier using this new, most-complex feature vector representation; and\n","3. Calculate and display the testing accuracy of the model.\n","\n","**QUESTION 10:**\n",">What is the testing accuracy for the model that was trained using the most complex feature vector representation (i.e., the feature vector representation that includes the TF-IDF scores)? Report your answer using three decimals of precision (e.g., 0.814)."]},{"cell_type":"code","metadata":{"id":"HZ5r5dfX6wn7"},"source":["#build training and testing feature vectors that contain the TF-IDF scores, unigrams, bigrams, POS unigrams, and POS bigrams\n","training_features = []\n","#for each row in the training dataframe\n","for row in df_train.itertuples():\n","  #combine the feature vectors\n","  feature_vector = np.append(row.unigram_probs, row.bigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_unigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  training_features.append(feature_vector)\n","\n","testing_features = []\n","#for each row in the testing dataframe\n","for row in df_test.itertuples():\n","  #combine the feature vectors\n","  feature_vector = np.append(row.unigram_probs, row.bigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_unigram_probs)\n","  feature_vector = np.append(feature_vector, row.pos_bigram_probs)\n","  #add the combined feature vector to the list\n","  testing_features.append(feature_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0bhoqxG7TNP"},"source":["#train the logistic regression classifier using the most complex training feature vectors\n","#(this will take about 75 seconds to run)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVatRR797Mek"},"source":["#calculate and display the testing accuracy for the most complex feature vector representation\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C31vJt63A9AJ"},"source":["**TASK 11:**\n",">Compare the performance of the logistic regression-based sentiment analyzer that you trained using the most complex feature vector representation with the performance of the lexicon-based sentiment analyzer from earlier in this lab assignment.\n","\n","**QUESTION 11:**\n",">Which of the following statements is correct?\n","* The logistic regression-based sentiment analyzer had better performance.\n","* The lexicon-based sentiment analyzer had better performance."]},{"cell_type":"markdown","metadata":{"id":"3vW2jSX7YEoi"},"source":["***TIP:*** After completing all of the tasks in this lab assignment, I would recommend selecting \"Restart and run all\" from the \"Runtime\" menu. Doing this will ensure that your results are not affected by issues relating to the random number generator.\n","\n","##End of Lab Assignment 04!"]}]}