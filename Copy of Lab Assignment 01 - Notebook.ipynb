{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Lab Assignment 01 - Notebook.ipynb","provenance":[{"file_id":"1SuhexWgOMpnHpOikhelvsajb8xNWYWZW","timestamp":1618781345419}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"qr7viTwm1ZQY"},"source":["#import libraries\n","import numpy as np #used to quickly perform mathematical calculations on vectors\n","import re #regular expressions; used to clean the text data\n","import sqlite3 #used to interact with the database\n","import pandas as pd #allows us to work with data using Pandas dataframes\n","from collections import Counter #used to quickly count letters and words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw4wmVM_zTyg"},"source":["#open a connection to the database\n","conn = sqlite3.connect('Project 01 - Database.db')\n","\n","#load all documents into a Pandas dataframe named 'df', and use the id column as the index\n","sql = 'SELECT * FROM Article'\n","df = pd.read_sql_query(sql, conn, index_col='id')\n","\n","#close database connection\n","conn.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZEhso_ywcu2","colab":{"base_uri":"https://localhost:8080/","height":137},"executionInfo":{"status":"ok","timestamp":1618776154236,"user_tz":420,"elapsed":559,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"07963e4e-b0be-44fe-b491-9d457cabb11f"},"source":["#define a function that will clean the raw input text in preparation for analysis\n","def clean_text(raw_text):\n","  #convert the raw text to lowercase\n","  text = raw_text.lower()\n","  #remove all numbers from the text using a regular expression\n","  text = re.sub(r'[0-9]', ' ', text)\n","  #remove all underscores from the text\n","  text = re.sub(r'\\_', ' ', text)\n","  #remove anything else in the text that isn't a word character or a space (e.g., punctuation, special symbols, etc.)\n","  text = re.sub(r'[^\\w\\s]', ' ', text)\n","  #remove any excess whitespace\n","  for _ in range(10):\n","    text = text.replace('  ', ' ')\n","  #remove any leading or trailing space characters\n","  text = text.strip()\n","  #return the clean text\n","  return text\n","\n","\n","#clean the raw text of each article, and store the resulting clean text in a new column \n","#in each dataframe. The code below uses a Python feature known as 'list comprehension'\n","#to quickly handle this task.\n","df['clean_text'] = [clean_text(raw_text) for raw_text in df.raw_text]\n","\n","\n","#show the cleaned text of the first English-language document\n","df.iloc[0]['clean_text']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'hiding women away in the home hidden behind veils is a backward view of islam president musharraf of pakistan has said during a visit to britain he was speaking to the bbc s newsnight programme a few hours before visiting the pakistani community in manchester my wife is travelling around she is very religious but she is very moderate said general musharraf it comes after pakistan s high commissioner to britain said some pakistanis should integrate more dr maleeha lodhi said people could not expect others to listen to their grievances if they isolated themselves gen musharraf told the bbc some people think that the women should be confined to their houses and put veils on and all that and they should not move out absolutely wrong the pakistani president was also asked whether he thought the war on terror had made the world less safe yes absolutely and i would add that unfortunately we are not addressing the core problems so therefore we can never address it in its totality he said we are fighting it in its immediate context but we are not fighting it in its strategic long term context it is the political disputes and we need to resolve them and also the issue of illiteracy and poverty this combined are breeding grounds of extremism and terrorism on monday the pakistani president met prime minister tony blair at downing street on his first official visit to london he is due to visit the pakistani community in manchester on tuesday afternoon the mirror newspaper said on tuesday it had been handed a sensitive dossier outling the details of gen musharraf s visit to britain the paper said the document had been found in a london street by a member of the public it said the dossier contained details about his movements and also confidential police radio channels call signs and codes speaking in london on monday gen musharraf said al qaeda was on the run in pakistan but standing next to mr blair he added that it was crucial to tackle the core of what creates terrorists what creates an extremist militant environment which then leads on to terrorism that is the resolution of political disputes mr blair said the two leaders had talked about afghanistan the wider war on terror the situation in the middle east and the ongoing dispute over kashmir we agreed that in afghanistan there is some cause for optimism about the progress that has been made there said mr blair in respect of iraq we agreed that whatever the issues of the past the important thing now is to see the strategy through and ensure that iraq is capable of becoming a stable and democratic state'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"3rDFv8h52JiI"},"source":["#build a vocabulary of words\n","all_text = ' '.join(df.clean_text) #join all of the English texts into one big string\n","words = all_text.split() #split the text into words\n","word_frequencies = Counter(words) #count all words in the text\n","vocabulary = list(word_frequencies.keys()) #get a list of all unique words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wVHW8Sg2GsV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618776318968,"user_tz":420,"elapsed":332,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"cc0be444-fa4c-4fc5-9b92-6f70cdac8e15"},"source":["#display the total number of unique words in the vocabulary\n","len(vocabulary)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27762"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"6JgNjbnY2g1c"},"source":["#define a class that we can use to hold information about each document\n","class Document:\n","  def __init__(self, id, category, word_frequencies, total_words):\n","    self.id = id #the document's unique ID number\n","    self.category = category #the document's topic\n","    self.predicted_category = None\n","    self.total_words = total_words #the total number of words in the document\n","    self.word_frequencies = word_frequencies #holds raw frequencies for each word in the vocabulary\n","    self.term_frequencies= None\n","    self.tfidf_scores = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyqQ7Dpr2-bk"},"source":["#sort the vocabulary to ensure that we all get consistent results!\n","vocabulary.sort()\n","#define a collection (list) to hold our Document objects\n","documents = []\n","#create a Document object for each document in the English-language corpus\n","for row in df.itertuples(): #for each row in the English-language dataframe\n","  words = row.clean_text.split() #split the (clean) text into words\n","  document_word_frequencies = Counter(words) #count all words in the document's (clean) text\n","  total_words = sum(document_word_frequencies.values()) #compute the total number of words in the document\n","  #compute the document's raw word frequencies for every word in the VOCABULARY (as opposed to every\n","  #word in the document). The vocabulary will contain more unique words than the document itself, but \n","  #we still need to consider EVERY word in the vocabulary, even if a particular word in the vocabulary \n","  #doesn't appear in the document. This will ensure that the feature vectors for all of the documents \n","  #are all exactly the same length and have exactly corresponding elements!\n","  vocabulary_word_frequencies = []\n","  for vocabulary_word in vocabulary:\n","    #if this vocabulary word exists in the document\n","    if vocabulary_word in document_word_frequencies:\n","      #add the raw document frequency for this vocabulary word to the collection\n","      vocabulary_word_frequencies.append(document_word_frequencies[vocabulary_word])\n","    else: #if this vocabulary word doesn't exist in the document\n","      #add a value of zero for this vocabulary word to the collection (since this\n","      #vocabulary word doesn't exist in the current document)\n","      vocabulary_word_frequencies.append(0)      \n","  #add a new Document object for this document to the collection\n","  documents.append(Document(row.Index, row.category, vocabulary_word_frequencies, total_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqcB27Tf33Ke","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618781315514,"user_tz":420,"elapsed":5370,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"a15723b9-5b04-4e83-d66a-61ff53b0c880"},"source":["#for each document in the 'documents' collection\n","for document in documents:\n","  #compute the unigram probability distributions for this document\n","  #document.term_frequencies = np.array(document.word_frequencies) / document.total_words\n","  document.term_frequencies = np.array(document.word_frequencies) / np.sqrt(document.total_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RsXooVnxqCOj"},"source":["#calc idf scores for each vocab word\n","idf_scores = []\n","for index in range(len(vocabulary)):\n","  number_of_documents_containing_word = 0\n","  for d in documents:\n","    if d.word_frequencies[index] > 0:\n","      number_of_documents_containing_word += 1\n","  idf = np.log(len(documents)/number_of_documents_containing_word)\n","  idf_scores.append(idf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLGQ2eI0r1lk"},"source":["idf_scores = np.array(idf_scores)\n","for d in documents:\n","  d.tfidf_scores = np.array(d.term_frequencies) * idf_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3V8roYr4JME"},"source":["#define a dictionary that holds each topic's name (keys) and average word probability distribution (values).\n","#The probability distributions are all numpy arrays of the same size as the vocabulary. All elements of each\n","#probability distribution are initialized to zero.\n","category_tfidf_scores = {'Business': np.zeros(len(vocabulary)), 'Politics': np.zeros(len(vocabulary)), 'Sports': np.zeros(len(vocabulary)), 'Technology': np.zeros(len(vocabulary)), 'Entertainment': np.zeros(len(vocabulary))}\n","#define a dictionary to hold the number of documents for each topic\n","document_counts = {'Business': 0, 'Politics': 0, 'Sports': 0, 'Technology': 0, 'Entertainment': 0}\n","#for each document in the corpus\n","for d in documents:\n","  #if the topic of this document is known\n","  if d.category != 'Unknown':\n","    #increment the document count for this topic\n","    document_counts[d.category] += 1\n","    #add this document's word probabilities to the running sum for the corresponding distribution \n","    #for the document's topic\n","    category_tfidf_scores[d.category] += d.tfidf_scores\n","#compute the average word probability distributions for each topic by dividing the summed probabilities\n","#by the number of documents for each topic\n","for category in category_tfidf_scores:\n","  category_tfidf_scores[category] /= document_counts[category]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8kB9X6B7-Tm"},"source":["#define a function to compute the Euclidean distance between two points \n","#(where each point is defined as a vector)\n","def get_distance(point1, point2):\n","  return np.sqrt(np.sum(np.square(point1 - point2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Kbr60Ng5OrR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618781330598,"user_tz":420,"elapsed":20420,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"fe40eadb-173c-4d31-9b4c-1f7a0da3a2b1"},"source":["#for each article\n","number_of_accurate_predictions = 0\n","number_of_known_articles = 0\n","for d in documents:\n","  min_distance = np.inf\n","  best_topic = None\n","  for category in category_tfidf_scores:\n","    distance = get_distance(d.tfidf_scores, category_tfidf_scores[category])\n","    if distance < min_distance:\n","        min_distance = distance\n","        best_category = category\n","  d.predicted_category = best_category\n","  if d.category != 'Unknown':\n","    number_of_known_articles += 1\n","    if d.category == d.predicted_category:\n","      number_of_accurate_predictions += 1\n","\n","print(number_of_accurate_predictions/number_of_known_articles)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in subtract\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["0.1836734693877551\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QmEnLawHztRZ"},"source":["with open('Connolly, Sean.csv', 'w') as csvfile:\n"," for d in documents:\n","   if d.category == 'Unknown':\n","    csvfile.write('{},{}\\n'.format(d.id, d.predicted_category))"],"execution_count":null,"outputs":[]}]}