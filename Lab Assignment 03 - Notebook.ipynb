{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab Assignment 03 - Notebook.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4vuiYfgqb5_0"},"source":["# Lab Assignment 03 - Automatic Text Summarization Using Python\n","\n","In this lab assignment, we will be using a variety of tools and techniques to automatically generate summaries of text documents.\n","\n","By the time you have completed this lab, you will have achieved all of the following learning objectives:\n","\n","## Learning Objectives\n","\n","* Gain familiarity with the Natural Language Toolkit (NLTK).\n","* Learn how to split text into paragraphs and sentences.\n","* Understand the challenges associated with sentence boundary disambiguation.\n","* Learn how to generate feature vector representations of sentences (as opposed to documents).\n","* Be able to compute a centroid for the sentences in a document.\n","* Know how to compute Maximum Marginal Relevance (MMR) scores, and use those MMR scores to generate an extractive text summary.\n","* Learn how to split a sentence into n-grams.\n","* Know how to construct a language model based on n-grams.\n","* Understand how to calculate conditional probabilities for n-grams, and how those probabilities can be used to select the next word in a sequence.\n","* Be able to generate an abstractive text summary.\n","* Continue to develop skills working with and analyzing text in Python."]},{"cell_type":"code","metadata":{"id":"_cFvIbmPCreF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621215215185,"user_tz":420,"elapsed":811,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"25a95adb-d4b1-4893-d7c8-f7ab269db354"},"source":["#import libraries\n","import nltk #the natural language toolkit\n","import numpy as np #used to generate random numbers\n","import pandas as pd #used to store data in a dataframe\n","import re #regular expressions; used to clean the text data\n","import string #used to determine if a character is a punctuation symbol\n","from nltk.probability import FreqDist #used to compute conditional frequency distributions\n","from nltk.tokenize import sent_tokenize, word_tokenize #used to split text into sentences, and to split sentences into words\n","from nltk.util import ngrams #used to generate n-grams for each sentences\n","from sklearn.feature_extraction.text import TfidfVectorizer #used to generate TF-IDF vectors and build the vocabulary\n","from sklearn.metrics.pairwise import cosine_similarity #used to compute cosine similarities\n","\n","#install nltk's 'punkt' tools, which are needed to tokenize sentences and words\n","nltk.download('punkt')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"omdaVDNLVrUw"},"source":["##Extractive Text Summarization\n","In ***extractive text summarization***, the summary consists of a sequence of words or sentences that have been selected and extracted from the original text. Extractive text summarization relies on *sentence vectors* (as opposed to document vectors) and similarity functions in order to create a summary of the original text.\n","\n","The basic approach to extractive text summarization involves two steps:\n","1. Split the original text into smaller sections or passages of text.\n","2. For each passage of text, compress its sentences into a smaller number of sentences. This is accomplished by selecting the most representative sentence within the passage of text.\n"]},{"cell_type":"markdown","metadata":{"id":"9ClNFgKcdTUN"},"source":["###Load Data\n","Run the code cell below to load the data for this part of the lab assignment. In this case, we'll be working with part of the Wikipedia article about the State of Washington."]},{"cell_type":"code","metadata":{"id":"X5-7deOYVutZ","executionInfo":{"status":"ok","timestamp":1621215280033,"user_tz":420,"elapsed":505,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#load raw text from a file\n","with open('Washington.txt', 'r') as f:\n","  text = f.read()"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxewGyy_d7DI"},"source":["###Split Text Into Passages\n","The first step in the extractive text summarization process is to split the raw text into passages. The simplest way of splitting text into passages is to rely on the text's existing paragraphs. Since paragraphs are used to divide a written document into smaller pieces that address a single topic or concept, using paragraphs as the basis for defining passages of text for our extractive summarization task is very natural. \n","\n","Paragraphs begin on a new line within a document, so we can use Python's new line character `\\n` (also called a *linefeed*) as a way of identifying the boundaries between paragraphs. \n","\n","**TASK 01**:\n",">Run the code cell below to split the raw text into paragraphs, then write a line of code that will display the total number of paragraphs in the raw text.\n","\n","**QUESTION 01**:\n",">How many paragraphs are in the article about the State of Washington?"]},{"cell_type":"code","metadata":{"id":"Ntz3Ty9RVv6o","executionInfo":{"status":"ok","timestamp":1621215327020,"user_tz":420,"elapsed":288,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#split text into paragraphs\n","text = text.replace('\\n\\n', '\\n') # replace any double linefeeds with a single linefeed\n","paragraphs = text.split('\\n')"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDh3Mhi9rvZ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621215330000,"user_tz":420,"elapsed":327,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"fd1624dd-345c-4766-a019-a62113d53504"},"source":["#display the number of paragraphs in the raw text\n","len(paragraphs)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"wxdo6YnWk6yY"},"source":["###Split Passages Into Sentences\n","As noted above, extractive text summarization relies on *sentence vectors* (as opposed to document vectors). As such, we'll need to split each passage (paragraph) of text into sentences. **Don't be fooled into thinking that this is an easy task!** Accurately identifying the boundaries between sentences is a much subtler process than it may initially seem. At first, we may think that English has only three punctuation symbols that can end a sentence -- a period (full stop) `.`, an exclamation point `!`, and a question mark `?` -- and that we can therefore identify the end of a sentence just by looking for one of these symbols. Things are, however, not so simple. Consider the following sentences:\n","* I enjoy books written by George R. R. Martin.\n","* \"What time will you arrive?\", she asked.\n","\n","If we used our simple rules, then the first sentence above would be treated as three separate sentences:\n","1. I enjoy books written by George R.\n","2. R.\n","3. Martin.\n","\n","...and the second sentence above would be treated as two separate sentences:\n","1. \"What time will you arrive?\n","2. \", she asked.\n","\n","As you can see, this problem is not as easy as it seems! The task of identifying the boundaries between sentences is known as ***sentence boundary disambiguation***. If you'd like to learn more, then please read [this article](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation).\n","\n","The good news is that researchers have created sophisticated tools that can accurately identify sentence boundaries. One of these tools is available in the **Natural Language Toolkit (NLTK)**, which we will use extensively in this lab assignment."]},{"cell_type":"markdown","metadata":{"id":"lWvxaNvKrDjq"},"source":["**TASK 02**:\n",">Run the code cell below to split each paragraph into sentences using NLTK's sentence tokenizer, then write some code that will calculate and display the average number of sentences per paragraph.\n","\n","**QUESTION 02**:\n",">What is the average number of sentences per paragraph for the article about the State of Washington? Report your answer using three decimals of precision (e.g., 3.456)."]},{"cell_type":"code","metadata":{"id":"AVuINHIlVv2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621215430709,"user_tz":420,"elapsed":468,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"393ac0e2-ac8a-42ca-b699-c074f510e50a"},"source":["#split each paragraph into sentences\n","for i in range(len(paragraphs)):\n","  paragraphs[i] = sent_tokenize(paragraphs[i])\n","\n","#display the sentences in the first paragraph\n","paragraphs[0]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Washington, officially the State of Washington, is a state in the Pacific Northwest region of the Western United States.',\n"," 'Named for George Washington, the first U.S. president, the state was made out of the western part of the Washington Territory, which was ceded by the British Empire in 1846, in accordance with the Oregon Treaty in the settlement of the Oregon boundary dispute.',\n"," 'The state, which is bordered on the west by the Pacific Ocean, Oregon to the south, Idaho to the east, and the Canadian province of British Columbia to the north, was admitted to the Union as the 42nd state in 1889.',\n"," 'Olympia is the state capital.',\n"," \"The state's largest city is Seattle.\",\n"," \"Washington is often referred to as Washington state to distinguish it from the nation's capital, Washington, D.C.\"]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"diUboXF1Vvza","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621215544991,"user_tz":420,"elapsed":567,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"79c8977a-e341-4cea-c765-27eafcb10a1a"},"source":["#calculate and display the average number of sentences per paragraph\n","for i in range(len(paragraphs)):\n","  print(len(paragraphs[i]))\n","  #4.4"],"execution_count":30,"outputs":[{"output_type":"stream","text":["6\n","5\n","5\n","2\n","4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fOhsPP0huimb"},"source":["###Move Data to a DataFrame\n","Next, let's move all of our sentences into a pandas dataframe so that they'll be easier to work with.\n","\n","Run the code cell below to add all of our sentences to a pandas dataframe. We'll also add a column named `sentence_id` that will record the order in which each sentence appeared in the original article, as well as a column named `paragraph_id` that will record the paragraph in which each sentence appeared in the original article."]},{"cell_type":"code","metadata":{"id":"L5ti75eDVvvZ","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1621215594340,"user_tz":420,"elapsed":321,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"b0135616-ad5f-43a9-d951-ac531184e01c"},"source":["#construct the rows of data\n","rows = []\n","sentence_id = 0\n","for paragraph_id in range(len(paragraphs)):\n","  for sentence in paragraphs[paragraph_id]:\n","    rows.append([sentence_id, paragraph_id, sentence])\n","    sentence_id += 1\n","\n","#add the sentences to a dataframe\n","df = pd.DataFrame(rows, columns=['sentence_id', 'paragraph_id', 'raw_text'])\n","df.set_index('sentence_id', inplace=True) #use the sentence_id column as the dataframe's index\n","\n","#display the first few rows\n","df.head(10)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paragraph_id</th>\n","      <th>raw_text</th>\n","    </tr>\n","    <tr>\n","      <th>sentence_id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Washington, officially the State of Washington...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Named for George Washington, the first U.S. pr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>The state, which is bordered on the west by th...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>Olympia is the state capital.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>The state's largest city is Seattle.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>Washington is often referred to as Washington ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>Washington is the 18th largest state, with an ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>Approximately 60 percent of Washington's resid...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>The remainder of the state consists of deep te...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>Washington is the second most populous state o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             paragraph_id                                           raw_text\n","sentence_id                                                                 \n","0                       0  Washington, officially the State of Washington...\n","1                       0  Named for George Washington, the first U.S. pr...\n","2                       0  The state, which is bordered on the west by th...\n","3                       0                      Olympia is the state capital.\n","4                       0               The state's largest city is Seattle.\n","5                       0  Washington is often referred to as Washington ...\n","6                       1  Washington is the 18th largest state, with an ...\n","7                       1  Approximately 60 percent of Washington's resid...\n","8                       1  The remainder of the state consists of deep te...\n","9                       1  Washington is the second most populous state o..."]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"FFD-wir-Ae2-"},"source":["###Clean Raw Text\n","As usual, we'll need to clean our raw text before we can compute a feature vector representation for each sentence.\n","\n","Run the code cell below to add our `get_clean_text()` function to your Python program."]},{"cell_type":"code","metadata":{"id":"lxTTSV7xVvn5","executionInfo":{"status":"ok","timestamp":1621215613435,"user_tz":420,"elapsed":415,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#define a function that will clean the raw input text in preparation for analysis. Returns a tuple containing\n","#both the cleaned text and the total number of words in the cleaned text.\n","def get_clean_text(raw_text):\n","  #find any period-separated acronyms (e.g., 'U.S.A', 'L.A.', etc.)\n","  period_separated_acronyms = re.findall(r'(?:[A-Z]\\.){2,}', raw_text)\n","  #remove periods from any period-separated acronyms\n","  for i in range(len(period_separated_acronyms)):\n","    acronym = period_separated_acronyms[i].replace('.', '')\n","    raw_text = raw_text.replace(period_separated_acronyms[i], acronym)\n","  #remove all numbers from the text using a regular expression\n","  text = re.sub(r'[0-9]', ' ', raw_text)\n","  #remove all underscores from the text\n","  text = re.sub(r'\\_', ' ', text)\n","  #remove anything else in the text that isn't a word character or a space (e.g., punctuation, special symbols, etc.)\n","  text = re.sub(r'[^\\w\\s]', ' ', text)\n","  #remove any excess whitespace\n","  for _ in range(10):\n","    text = text.replace('  ', ' ')\n","  #remove any leading or trailing space characters\n","  text = text.strip()\n","  #split the text into a list of words\n","  words = text.split()\n","  #convert all non-acronyms to lowercase\n","  for i in range(len(words)): #for each index in the words collection\n","    word = words[i] #define the current word\n","    if len(word) > 1 and len(word) < 7: #if this word is two to six characters long\n","      if word.isupper() == False: #if at least one character in this word is not uppercase\n","        #this word is not an acronym because it is not all uppercase, so convert it to lowercase\n","        words[i] = word.lower()\n","    else: #this word is not an acronym because it consists of one letter or more than six letters, so convert it to lowercase\n","      words[i] = word.lower()\n","  #return the cleaned text and the number of words in the cleaned text\n","  return (' '.join(words), len(words))"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1SZ0HobBptP"},"source":["Now that the `get_clean_text()` function has been made available to your Python program, you're ready to clean the raw text of each sentence.\n","\n","Run the code cell below to clean the raw text of each sentence and save the resulting cleaned text and the total number of words in the sentence to new columns in the dataframe."]},{"cell_type":"code","metadata":{"id":"C7f6Aas4Vvek","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1621215638541,"user_tz":420,"elapsed":285,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"61bafa16-72d9-4e51-bb21-837527c48e56"},"source":["#clean the raw text of each sentence and save the resulting cleaned text and total number of words for\n","#each sentence in new dataframe columns named 'clean_text' and 'total_words'.\n","df[['clean_text', 'total_words']] = [get_clean_text(raw_text) for raw_text in df.raw_text]\n","\n","#show the first few rows in the dataframe\n","df.head()"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paragraph_id</th>\n","      <th>raw_text</th>\n","      <th>clean_text</th>\n","      <th>total_words</th>\n","    </tr>\n","    <tr>\n","      <th>sentence_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Washington, officially the State of Washington...</td>\n","      <td>washington officially the state of washington ...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Named for George Washington, the first U.S. pr...</td>\n","      <td>named for george washington the first US presi...</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>The state, which is bordered on the west by th...</td>\n","      <td>the state which is bordered on the west by the...</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>Olympia is the state capital.</td>\n","      <td>olympia is the state capital</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>The state's largest city is Seattle.</td>\n","      <td>the state s largest city is seattle</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             paragraph_id  ... total_words\n","sentence_id                ...            \n","0                       0  ...          19\n","1                       0  ...          43\n","2                       0  ...          40\n","3                       0  ...           5\n","4                       0  ...           7\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"J7jkfIF6kmvh"},"source":["###Sentence Length\n","Research has shown that there is an inverse relationship between the number of words in a sentence and how easily a typical reader can understand the sentence. Specifically:\n","* Sentences containing approximately 11 words are rated as *easy to understand*.\n","* Sentences containing approximately 21 words are rated as *somewhat difficult to understand*.\n","* Sentences containing approximately 25 words are rated as *difficult to understand*.\n","* Sentences containing approximately 29 words or more are rated as *very difficult to understand*.\n","\n","In light of this information, let's evaluate the average sentence length among the sentences in our article about the State of Washington.\n","\n","**TASK 03**:\n",">Write a line of code in the cell below that will display the average number of words per sentence.\n","\n","**QUESTION 03**:\n",">What is the average number of words per sentence among the sentences in the article about the State of Washington? Report your answer using three decimals of precision (e.g., 18.678)."]},{"cell_type":"code","metadata":{"id":"iNPIoyLfChOL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621216007299,"user_tz":420,"elapsed":287,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"43c073f7-4ed1-4524-c35c-46aa0b835feb"},"source":["#display the average number of words per sentence\n","for i in df:\n","  #print(raw_text['total_words'])\n","  print(df['total_words'])\n","  #24.636"],"execution_count":44,"outputs":[{"output_type":"stream","text":["sentence_id\n","0     19\n","1     43\n","2     40\n","3      5\n","4      7\n","5     18\n","6     23\n","7     40\n","8     40\n","9     19\n","10    25\n","11     6\n","12    19\n","13    36\n","14    29\n","15     9\n","16    22\n","17    26\n","18    14\n","19    13\n","20    59\n","21    30\n","Name: total_words, dtype: int64\n","sentence_id\n","0     19\n","1     43\n","2     40\n","3      5\n","4      7\n","5     18\n","6     23\n","7     40\n","8     40\n","9     19\n","10    25\n","11     6\n","12    19\n","13    36\n","14    29\n","15     9\n","16    22\n","17    26\n","18    14\n","19    13\n","20    59\n","21    30\n","Name: total_words, dtype: int64\n","sentence_id\n","0     19\n","1     43\n","2     40\n","3      5\n","4      7\n","5     18\n","6     23\n","7     40\n","8     40\n","9     19\n","10    25\n","11     6\n","12    19\n","13    36\n","14    29\n","15     9\n","16    22\n","17    26\n","18    14\n","19    13\n","20    59\n","21    30\n","Name: total_words, dtype: int64\n","sentence_id\n","0     19\n","1     43\n","2     40\n","3      5\n","4      7\n","5     18\n","6     23\n","7     40\n","8     40\n","9     19\n","10    25\n","11     6\n","12    19\n","13    36\n","14    29\n","15     9\n","16    22\n","17    26\n","18    14\n","19    13\n","20    59\n","21    30\n","Name: total_words, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q3_A5xnbofwt"},"source":["### Compute TF-IDF Scores & Build the Vocabulary\n","Now we're ready to compute the TF-IDF scores for each sentence. Note that in previous assignments, we've computed TF-IDF scores for each *article* in a *corpus*. Our current task is conceptually similar, except that we'll be computing a TF-IDF vector for each *sentence* within an *article*. Each of these feature vectors, then, will describe the semantic content of its associated sentence relative to the article as a whole. This approach is necessary because extractive text summarization relies on snetence-level vectors instead of document-level vectors. \n","\n","Run the code cell below to build the vocabulary and compute and add a TF-IDF vector for each sentence to the dataframe."]},{"cell_type":"code","metadata":{"id":"YU9H6yXuCg-7","colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"status":"ok","timestamp":1621216166766,"user_tz":420,"elapsed":306,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"f7552346-541e-4f0d-fe52-b816974cbf66"},"source":["#build the vocabulary of unique words and compute TF-IDF scores for each sentence\n","vectorizer = TfidfVectorizer(lowercase=False)\n","sentence_tfidf_scores = np.array(vectorizer.fit_transform(df.clean_text).todense())\n","vocabulary = vectorizer.vocabulary_\n","\n","#add each sentence's vector of TF-IDF scores to the dataframe\n","df['tfidf_scores'] = [tfidf_scores for tfidf_scores in sentence_tfidf_scores]\n","\n","#display the first few rows\n","df.head()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paragraph_id</th>\n","      <th>raw_text</th>\n","      <th>clean_text</th>\n","      <th>total_words</th>\n","      <th>tfidf_scores</th>\n","    </tr>\n","    <tr>\n","      <th>sentence_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Washington, officially the State of Washington...</td>\n","      <td>washington officially the state of washington ...</td>\n","      <td>19</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Named for George Washington, the first U.S. pr...</td>\n","      <td>named for george washington the first US presi...</td>\n","      <td>43</td>\n","      <td>[0.0, 0.1316039800009417, 0.0, 0.0, 0.16478488...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>The state, which is bordered on the west by th...</td>\n","      <td>the state which is bordered on the west by the...</td>\n","      <td>40</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1708030013569...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>Olympia is the state capital.</td>\n","      <td>olympia is the state capital</td>\n","      <td>5</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>The state's largest city is Seattle.</td>\n","      <td>the state s largest city is seattle</td>\n","      <td>7</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             paragraph_id  ...                                       tfidf_scores\n","sentence_id                ...                                                   \n","0                       0  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","1                       0  ...  [0.0, 0.1316039800009417, 0.0, 0.0, 0.16478488...\n","2                       0  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1708030013569...\n","3                       0  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","4                       0  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"jMIsC62iqpa6"},"source":["###Compute a Centroid Representation of the Article\n","Next, we'll compute the centroid of the TF-IDF vectors for all of the sentences in the article. This centroid vector will be the average of all of the sentence-level TF-IDF vectors, and will hence represent the semantic content of the article as a whole. This step is necessary because we'll be using **Maximum Marginal Relevance (MMR)** to identify the most representative sentence from each passage of text, and MMR works by choosing the sentence from each passage that is most similar to the document overall, while minimizing redundancy among the set of chosen sentences.\n","\n","Run the code cell below to compute a centroid vector for all of the sentences in the article."]},{"cell_type":"code","metadata":{"id":"utRiXdUjChGs","executionInfo":{"status":"ok","timestamp":1621216284259,"user_tz":420,"elapsed":438,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#compute the centroid of the TF-IDF vectors for all of the sentences in the article\n","centroid = np.reshape(np.mean(df.tfidf_scores, axis=0), (1, -1))"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bNGa-2oLjMX"},"source":["###Calculating Maximum Marginal Relevance (MMR)\n","Next, let's implement a function that will identify the sentence with the Maximum Marginal Relevance (MMR) score in each paragraph. The MMR formula is used to assign each sentence in a paragraph a score based on:\n","1. How similar the sentence is to the article as a whole (i.e., to the article's centroid); and\n","2. How redundant the semantic content of each sentence is relative to the semantic content of any already-chosen summary sentences.\n","\n","The idea with MMR is thus to find a group of sentences that summarize the overall article without being excessively redundant, which seems to be a reasonable strategy.\n","\n","Run the code cell below to add the `get_extractive_summary()` funtion to your Python program. Note that this function has a `lambda_` parameter that controls the level of relevance vs. redundancy among the sentences in the summary. Values of this parameter can range from 0.0 to 1.0. Smaller values of this parameter yield a summary whose sentences are more similar to the article as a whole, but which may also contain more redundant information. Larger values, by contrast, will yield a summary whose sentences will be less cohesive, but which will express a wider variety of content."]},{"cell_type":"code","metadata":{"id":"1WntI1ifCg3k","executionInfo":{"status":"ok","timestamp":1621216387009,"user_tz":420,"elapsed":310,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#define a function that will generate an extractive summary of the article's text by identifying the best \n","#representative, non-redundant sentence in each paragraph; i.e., the sentence with the Maximum Marginal Relevance (MMR).\n","def get_extractive_summary(lambda_):\n","  #NOTE: the lambda parameter controls the level of relevance vs. redundancy among the sentences in the summary\n","  summary_sentences = []\n","  for paragraph_id in range(len(paragraphs)): #for each paragraph ID\n","    #extract the TF-IDF scores for this paragraph's sentences from the dataframe\n","    df_sentences = df[['tfidf_scores']][df.paragraph_id == paragraph_id]\n","    #identify the sentence with the Maximum Marginal Relevance (MMR) for this paragraph\n","    maximum_marginal_relevance = -np.inf #holds the MMR for the sentences in this paragraph\n","    best_sentence = None #holds the sentence with the MMR\n","    for sentence in df_sentences.itertuples(): #for each sentence in this paragraph\n","      #calculate the cosine similarity between this sentence's TF-IDF scores and the article's centroid\n","      similarity_to_article = cosine_similarity(np.reshape(sentence.tfidf_scores, (1, -1)), centroid)[0][0]\n","      #calculate the maximum cosine similarity between this sentence and any already-chosen summary sentences\n","      max_similarity_to_summary_sentence = -np.inf\n","      for sentence_id, summary_sentence_tfidf_scores in summary_sentences:\n","        similarity_to_summary_sentence = cosine_similarity(np.reshape(sentence.tfidf_scores, (1, -1)), np.reshape(summary_sentence_tfidf_scores, (1, -1)))[0][0]\n","        if similarity_to_summary_sentence > max_similarity_to_summary_sentence:\n","          max_similarity_to_summary_sentence = similarity_to_summary_sentence\n","      #compute the marginal relevance for this sentence\n","      marginal_relevance = ((1 - lambda_) * similarity_to_article) - (lambda_ * max_similarity_to_summary_sentence)\n","      if marginal_relevance > maximum_marginal_relevance:\n","        maximum_marginal_relevance = marginal_relevance\n","        best_sentence = (sentence.Index, sentence.tfidf_scores)\n","    #add the sentence with the Maximum Marginal Relevance (MMR) for this paragraph to the collection of summary sentences\n","    summary_sentences.append(best_sentence)\n","\n","  #construct and return the summary of the article\n","  article_summary = ''\n","  for sentence_id, _ in summary_sentences:\n","    article_summary += df.iloc[sentence_id]['raw_text'] + ' '\n","  return article_summary.strip()\n"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOkvUfElKbZ-"},"source":["###Generate Extractive Text Summaries\n","We're finally ready to generate extractive summaries of our article about the State of Washington. Yay!\n","\n","**TASK 04**:\n",">Write a line of code in the cell below that will generate an extractive summary of the article about the State of Washington using a value of the `lambda_` parameter of 0.2. This value will yield a summary that emphasizes similarity between each sentence and the article as a whole.\n","\n","**QUESTION 04**:\n",">Which topics are mentioned in the extractive summary when the value of the lambda parameter is 0.2?\n","* Agricultural products grown in Washington, such as apples, hops, and pears.\n","* Mount Rainier.\n","* Manufacturing industries in Washington, such as aircraft and shipbuilding.\n","* Wine production in Washington.\n","* Washington's high life expectancy and low unemployment."]},{"cell_type":"code","metadata":{"id":"nKJnDIRrCguO","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1621216460850,"user_tz":420,"elapsed":281,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"e09bf8c6-2181-4742-d9ea-e341cb624932"},"source":["#display a summary of the article using a value of 0.2 for the lambda parameter\n","get_extractive_summary(.2)"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Washington, officially the State of Washington, is a state in the Pacific Northwest region of the Western United States. Washington is the second most populous state on the West Coast and in the Western United States, after California. Washington is the nation's largest producer of apples, hops, pears, red raspberries, spearmint oil, and sweet cherries, and ranks high in the production of apricots, asparagus, dry edible peas, grapes, lentils, peppermint oil, and potatoes. Manufacturing industries in Washington include aircraft and missiles, shipbuilding, and other transportation equipment, food processing, metals and metal products, chemicals, and machinery. Washington is one of the wealthiest and most socially liberal states in the country.\""]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"yqRG4YXPZjqa"},"source":["**TASK 05**:\n",">Write a line of code in the cell below that will generate an extractive summary of the article about the State of Washington using a value of the `lambda_` parameter of 0.8. This value will yield a summary that emphasizes greater diversity among the topics that appear in the summary.\n","\n","**QUESTION 05**:\n",">Which topics are mentioned in the extractive summary when the value of the lambda parameter is 0.8?\n","* Agricultural products grown in Washington, such as apples, hops, and pears.\n","* Mount Rainier.\n","* Manufacturing industries in Washington, such as aircraft and shipbuilding.\n","* Wine production in Washington.\n","* Washington's high life expectancy and low unemployment."]},{"cell_type":"code","metadata":{"id":"Y73fiWfUKBTC","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1621216531390,"user_tz":420,"elapsed":295,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"62bdcb78-b305-4c10-a9fd-c831d0bfad4a"},"source":["#display a summary of the article using a value of 0.8 for the lambda parameter\n","get_extractive_summary(.8)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Washington, officially the State of Washington, is a state in the Pacific Northwest region of the Western United States. Mount Rainier, an active stratovolcano, is the state's highest elevation, at almost 14,411 feet, and is the most topographically prominent mountain in the contiguous U.S. Washington ranks second only to California in wine production. Manufacturing industries in Washington include aircraft and missiles, shipbuilding, and other transportation equipment, food processing, metals and metal products, chemicals, and machinery. The state consistently ranks among the best for life expectancy and low unemployment.\""]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"sKGrWgvTVhSR"},"source":["##Abstractive Text Summarization\n","We'll next turn our attention to ***abstractive text summarization***, in which our goal is to generate new sentences that are semantically similar to the original text, but which did not appear in the original text. For this lab assignment, we'll be using n-gram language models and conditional probabilities for this purpose. If each word appears in the summary text with approximately the same probability as it appeared in the original text, then the generated summary text can be said to approximate the original text."]},{"cell_type":"markdown","metadata":{"id":"XXrSV-VBfbFa"},"source":["### Load Data\n","For this part of the lab assignment, we'll be using the complete text of the books *The Adventures of Sherlock Holmes* and *The Memoirs of Sherlock Holmes* as the basis for generating our abstractive summaries. \n","\n","After uploading the `Sherlock Holmes.txt` file, run the code cell below to make the text of these two Sherlock Holmes books available to your Python program. This code cell will also clean up the linefeed characters in the original text, and will use NLTK's sentence tokenizer to split the text into sentences."]},{"cell_type":"code","metadata":{"id":"q7PPnEdQC2hf","executionInfo":{"status":"ok","timestamp":1621216684142,"user_tz":420,"elapsed":824,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#load raw text from a file\n","with open('Sherlock Holmes.txt', 'r') as f:\n","  text = f.read()\n","\n","#replace linefeed characters\n","text = text.replace('\\n\\n', ' ')\n","text = text.replace('\\n', ' ')\n","\n","#split text into a list of sentences\n","sentences = sent_tokenize(text)"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOqw6u2fgouM"},"source":["**TASK 06**:\n",">Write a line of code in the cell below that will display the total number of sentences in the Sherlock Holmes books.\n","\n","**QUESTION 06**:\n",">How many sentences are in the Sherlock Holmes books?"]},{"cell_type":"code","metadata":{"id":"4ogR7RMoDNW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621216704411,"user_tz":420,"elapsed":282,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"57f5a11d-8ab3-437e-be9a-de40d0cbc026"},"source":["#display the total number of sentences in the Sherlock Holmes books\n","print(len(sentences))"],"execution_count":52,"outputs":[{"output_type":"stream","text":["9097\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ULAm7zEXmJIO"},"source":["###Tokenize Sentences Into Lists\n","Next, we'll tokenize each sentence into a list whose elements contain the words and symbols that together comprise the sentence. This is necessary because NLTK's `ngrams()` function (which we'll use to generate n-grams for our language models) requires input sentences to be in the form of tokenized lists.\n","\n","Run the code cell below to convert all of the sentences from the Sherlock Holmes books into lists by using NLTK's word tokenizer."]},{"cell_type":"code","metadata":{"id":"zWEs6MXvG5q4","colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"status":"error","timestamp":1621217509978,"user_tz":420,"elapsed":9913,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"6d7aa3b7-45ea-485b-fcba-6718638f4b65"},"source":["#tokenize each sentence into a list of words and symbols\n","sentences = [word_tokenize(sentence) for sentence in sentences]"],"execution_count":74,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-7b126febdb78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenize each sentence into a list of words and symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-74-7b126febdb78>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenize each sentence into a list of words and symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"]}]},{"cell_type":"markdown","metadata":{"id":"6tjL4qn2nQyg"},"source":["**TASK 07**:\n",">Write some code in the cell below that will compute and display the average number of words per sentence for the sentences in the Sherlock Holmes books.\n","\n","**QUESTION 07**:\n",">How does the average number of words per sentence in the Sherlock Holmes books compare to the average number of words per sentence in the article about the State of Washington?\n","* On average, the Sherlock Holmes books have more words per sentence than the article about the State of Washington.\n","* On average, the Sherlock Holmes books have fewer words per sentence than the article about the State of Washington."]},{"cell_type":"code","metadata":{"id":"KUaElZYHHPxC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621217592500,"user_tz":420,"elapsed":299,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"4016c0e5-29df-476a-e471-9adba39c8b98"},"source":["#calculate and display the average number of words per sentence in the Sherlock Holmes books\n","sum = 0\n","for i in range(len(sentences)-1):\n","  sum += len(sentences[i])\n","print(sum/9097)\n","#26.256 - more words in sherlock holmes"],"execution_count":76,"outputs":[{"output_type":"stream","text":["26.256238320325384\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0mUDibjTplAw"},"source":["###Convert Sentences to N-Grams\n","Next, let's write a function that will convert a tokenized sentence into a list of n-grams of size `n`. Thus far in our course we've focused primarily on unigrams (1-grams), but these 1-grams do not encode any context because they are just single words. To add more context, we can consider short sequences of words. For example, with `n = 2`, we can capture more subtle concepts such as *bad movie* or *dark chocolate*. With `n = 3` we can capture even more subtle concepts such as *My favorite book* or *I like Python*. Using these short sequences of words, we will be able to calculate the probability of any word being the next word to appear in the sequence. For example, what word most likely comes next in this sequence?: *My favorite book ________*\n","\n","Run the code cell below to define a function that will convert a tokenized sentence into a list of n-grams of size `n`. Note that the function left-pads each sentence with an appropriate number of `<s>` start-of-sentence tags, depending on the value of `n`. This will allow us to figure out the probability of any word being the first word in a sentence. We also append an `</s>` end-of-sentence tag to the end of every sentence so that we will be able to identify n-grams that appear at the end of sentences. Note also that the n-grams are stored as tuples with the format: `((n - 1 previous words), next word in sequence)`. This format will be very useful when we begin calculating the probabailities of different words appearing next in a sequence."]},{"cell_type":"code","metadata":{"id":"Jn2ernJNJLQq","executionInfo":{"status":"ok","timestamp":1621217661304,"user_tz":420,"elapsed":343,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#define a function that will convert a tokenized sentence into a list of n-grams of size n.\n","def get_ngrams(sentence, n):\n","  #left-pad the sentence with an appropriate number of start-of-sentence tags. This is necessary\n","  #to allow the conditional probability of the first word in each sentence to be computed.\n","  sentence = (n - 1) * ['<s>'] + sentence\n","  #right-pad the sentence with an end-of-sentence tag. This is necessary to allow the conditional\n","  #probability of ending the sentence to be computed.\n","  sentence.append('</s>')\n","  #generate a list of n-grams for the sentence\n","  n_grams = list(ngrams(sentence, n))\n","  #convert n-grams into tuples with the format: ((n - 1 previous words), next word in sequence)\n","  n_grams = [((n_gram[:-1]), n_gram[-1]) for n_gram in n_grams]\n","  return n_grams"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M90nADO8t3aR"},"source":["**TASK 08**:\n",">Write a line of code in the cell below that will display all of the 2-grams (i.e., `n = 3`) for the sentence at index location 999 within the `sentences` collection.\n","\n","**QUESTION 08**:\n",">What is the first 3-gram for the sentence that appears at index location 999 in the `sentences` collection?"]},{"cell_type":"code","metadata":{"id":"gf675D-4LZXL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621217700581,"user_tz":420,"elapsed":486,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"f5c039a4-b678-44c0-d830-62414a3d7920"},"source":["#display all of the 3-grams for the sentence that appears at index location 999 in the 'sentences' list\n","get_ngrams(sentences[999],3)"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('<s>', '<s>'), 'You'),\n"," (('<s>', 'You'), 'have'),\n"," (('You', 'have'), 'really'),\n"," (('have', 'really'), 'done'),\n"," (('really', 'done'), 'very'),\n"," (('done', 'very'), 'well'),\n"," (('very', 'well'), 'indeed'),\n"," (('well', 'indeed'), '.'),\n"," (('indeed', '.'), '</s>')]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"9aVJeYscwFda"},"source":["###Define Punctuation Symbols\n","Next, we'll define what constitutes a punctuation symbol within our text. Since we're not cleaning this text in the same way that we did when working with feature vectors, all of the punctuation symbols remain in the text. For the most part, we can use Python's standard set of punctuation symbols, but we'll need to add a few more to accommodate directional closing double-quotes and single-quotes.\n","\n","Run the code cell below to define the punctuation symbols."]},{"cell_type":"code","metadata":{"id":"j8hT-MV1CMQU","executionInfo":{"status":"ok","timestamp":1621217984438,"user_tz":420,"elapsed":449,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#define punctuation symbols\n","punctuation = string.punctuation + ''"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1pLqac0xF9P"},"source":["###Define a Language Model Class\n","We've now reached a point where we can define a class that will allow us to construct and work with language models based on n-grams. Each language model is constructed based on a set of input sentences and word\n","sequences (n-grams) of length `n`. The language model consists of a collection of word sequences of length \n","(n - 1) and the set of possible words that might immediately follow each word sequence. Each candidate word that might follow a particular sequence of words has a specific probability of being the next word in the sequence.\n","\n","Run the code cell below to add the `Language_Model()` class to your Python project.\n","\n","***Study the code and the comments in this class very carefully so that you will understand how the language model is built and how the conditional probabilities are calculated!***"]},{"cell_type":"code","metadata":{"id":"EjF1u5s9VOYR","executionInfo":{"status":"ok","timestamp":1621217991820,"user_tz":420,"elapsed":444,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#Define a language model class. A language model is constructed based on a set of input sentences and word\n","#sequences (n-grams) of length n. The language model consists of a collection of word sequences of length \n","#(n - 1) and the set of possible words that might follow each word sequence. Each candidate word that might \n","#follow a particular sequence of words has a specific probability of being the next word in the sequence.\n","class Language_Model():\n","  #define the class's initialization function\n","  def __init__(self, n, sentences): #n = the size of n-grams to use; sentences = the tokenized sentences from which to construct the language model.\n","    self.end_of_sentence_tag = '</s>' #the tag that marks the end of a sentence\n","    self.n = n #the number of words that constitute an n-gram\n","    self.n_gram_count = {} #a dictionary that holds the number of times each n-gram has been observed in the text. Keys = n-grams, values = n-gram counts\n","    self.sentences = sentences #the sentences from which to construct the language model\n","    self.start_of_sentence_sequence = tuple(['<s>' for _ in range(n - 1)]) #a tuple containing the appropriate number of successive start-of-sentence tags (based on n) to indicate the beginning of a new sentence\n","    self.word_sequences = {} #a dictionary containing sequences of words (keys) and a list of candidate words that may follow each sequence (values)\n","    \n","    #build the language model\n","    for sentence in sentences: #for each sentence\n","      #get the n-grams for this sentence\n","      n_grams = get_ngrams(sentence, n)\n","      #for each n-gram in this sentence\n","      for n_gram in n_grams:\n","        #if this n-gram has been seen previously\n","        if n_gram in self.n_gram_count:\n","          self.n_gram_count[n_gram] += 1 #increment the number of times this n-gram has been observed\n","        else: #if this is the first time this n-gram has been seen\n","          self.n_gram_count[n_gram] = 1 #add this n-gram to the dictionary and set its number of observations to 1\n","        #extract the sequence of previous words and the next word from this n-gram\n","        word_sequence, next_word = n_gram\n","        #if this sequence of words has already been observed\n","        if word_sequence in self.word_sequences:\n","          self.word_sequences[word_sequence].append(next_word) #add the next word to this sequence's list of next words\n","        else: #if this sequence of words has not yet been observed\n","          self.word_sequences[word_sequence] = [next_word] #add this word sequence to the dictionary and initialize its list of next words\n","    \n","    #calculate conditional probabilities for all of the words that might follow each sequence of words\n","    for word_sequence in self.word_sequences:\n","      #get the raw word frequencies for each word that might follow this word sequence\n","      freq_dist = FreqDist(self.word_sequences[word_sequence])\n","      #compute the conditional probabilities for each word and store them in a list that is sorted\n","      #from the most probable word to the least probable word\n","      word_probabilities = [(word, freq_dist.freq(word)) for word, frequency in freq_dist.most_common()]\n","      #replace the previous list of words that might follow this word sequence with the list of words and probabilities\n","      self.word_sequences[word_sequence] = word_probabilities\n","\n","  #define a function that will return the next word (or symbol) in a sequence. The likelihood of any of the\n","  #possible words being returned depends on its conditional probability, given the word sequence.\n","  def get_next_word(self, word_sequence):\n","    #get a random number between 0 and 1 to serve as the probability threshold\n","    random_probability_threshold = np.random.random()\n","    #define a variable to hold the cumulative probability\n","    cumulative_probability = 0.0\n","    #define a variable to hold the next word in the sequence\n","    next_word = None\n","    #determine which word should appear next in the sequence based on the words' conditional probabilities,\n","    #for each possible next word\n","    for word, word_probability in self.word_sequences[word_sequence]:\n","      #add this word's probability of being next in the sequence to the cunulative probability\n","      cumulative_probability += word_probability\n","      #if the cumulative probability exceeds the randomly chosen probability threshold\n","      if cumulative_probability >= random_probability_threshold:\n","        #assign the current word to be the next word in the sequence, and exit the loop immediately\n","        next_word = word\n","        break\n","    return next_word\n","  \n","  #define a function that will generate a sentence based on the language model\n","  def generate_sentence(self):\n","    #define a variable to hold the words (and symbols) that comprise the sentence\n","    sentence = ''\n","    #initialize the word sequence to the start-of-sentence sequence\n","    word_sequence = self.start_of_sentence_sequence\n","    #get the first word in the sentence\n","    word = self.get_next_word(word_sequence)\n","    #while the end of the sentence has not yet been reached\n","    while word != self.end_of_sentence_tag:\n","      #add this word (or symbol) to the sentence\n","      sentence += ' ' + word\n","      #construct the next word sequence\n","      word_sequence = word_sequence[1:] + (word,)      \n","      #get the next word (or symbol) in the sentence\n","      word = self.get_next_word(word_sequence)\n","    #cleanup punctuation and spacing\n","    for symbol in punctuation:\n","      sentence = sentence.replace(' ' + symbol, symbol)\n","    for symbol in '':\n","      sentence = sentence.replace(symbol + ' ', symbol)\n","    sentence = sentence.replace('n t', 'nt')\n","    sentence = sentence.replace(' s ', 's ')\n","    sentence = sentence.replace(' m ', 'm ')\n","    return sentence.strip()\n"],"execution_count":80,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zo2sW1bpzm-c"},"source":["###Generate Language Models\n","We now have everything that we need to generate language models based on n-grams. Yay!\n","\n","Run the code cell below to generate a language model using 3-grams (i.e., `n = 3`)."]},{"cell_type":"code","metadata":{"id":"BOMgG6mweDmO","executionInfo":{"status":"ok","timestamp":1621218421342,"user_tz":420,"elapsed":2188,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}}},"source":["#generate a language model using 3-grams\n","n3_model = Language_Model(3, sentences)\n"],"execution_count":88,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFUGwJQ00XSE"},"source":["**TASK 09**:\n",">Write a line of code in the cell below that will display the words that are most likely to appear as the first word in a sentence in the Sherlock Holmes books. ***Tip***: You will need to get the values out of the `n3_model` language model's `word_sequences[]` collection for the language model's `start_of_sentence_sequence` property.\n","\n","**QUESTION 09**:\n",">Which <u>word</u> (as opposed to a punctuation symbol) is most likely to appear as the first word in a sentence in the Sherlock Holmes books?\n","* It\n","* The\n","* I\n","* You"]},{"cell_type":"code","metadata":{"id":"kd07xm_3e-NW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621218918363,"user_tz":420,"elapsed":273,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"886d6523-d1d8-49f1-e61b-d81ea0114dd6"},"source":["#display the words that are most likely to appear as the first word in a sentence\n","x = n3_model.start_of_sentence_sequence\n","t=1\n","it_count = 0\n","the_count = 0\n","i_count = 0\n","you_count = 0\n","while t < 1000:\n","  if (n3_model.get_next_word(x)) == 'It':\n","    it_count += 1\n","  if (n3_model.get_next_word(x)) == 'The':\n","    the_count += 1\n","  if (n3_model.get_next_word(x)) == 'I':\n","    i_count += 1\n","  if (n3_model.get_next_word(x)) == 'You':\n","    you_count += 1\n","  t+=1\n","print(it_count)\n","print(the_count)\n","print(i_count)\n","print(you_count)\n","#"],"execution_count":111,"outputs":[{"output_type":"stream","text":["47\n","71\n","94\n","23\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RcggTZH93a83"},"source":["###Generate Abstractive Sentences Using the Language Model\n","Now it's time to use our language model to generate some new sentences that don't appear in the original text. Nevertheless, our new sentences should seem  similar to the kinds of sentences that we would expect to appear in a Sherlock Holmes book, given that the model was trained using Sherlock Holmes books.\n","\n","Run the code cell below to generate five new sentences using our `n = 3` language model. Since the value of `n` is relatively small in this particular language model, we should not expect the sentences to make perfect sense."]},{"cell_type":"code","metadata":{"id":"K_EbXMbSUPff","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"error","timestamp":1621210526047,"user_tz":420,"elapsed":473,"user":{"displayName":"Sean Connolly","photoUrl":"","userId":"15606364408941710004"}},"outputId":"a1d9ab82-3652-44a1-d847-12b8822d6455"},"source":["#set the seed for the random number generator (to ensure consistent results)\n","np.random.seed(123)\n","\n","#generate five sentences for the n=3 language model\n","for i in range(5):\n","  sentence = n3_model.generate_sentence()\n","  print(sentence, '\\n')"],"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-19e8dc988db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#generate five sentences for the n=3 language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn3_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'n3_model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"wFUACWcG5SW_"},"source":["One of the interesting things about language models based on n-grams is that as the value of `n` increases, the newly generated sentences seem more and more natural. The problem, however, is that as the value of `n` increases, so too does the probability of exactly reproducing one of the actual sentences from the original text. For this reason, it is generally recommended to use values of `n` of between 3 and 5.\n","\n","**TASK 10**:\n",">Write some code in the cell below that will train a language model for `n = 5`, and then generate five new sentences using your `n = 5` language model. ***NOTE***: Do <u>not</u> change the random seed value that appears in the cell. If you do, you will be unable to answer Question 10.\n","\n","**QUESTION 10**:\n",">What is the fifth sentence generated by the `n = 5` model?"]},{"cell_type":"code","metadata":{"id":"VVFdqNGPUQlK"},"source":["#generate a language model using 5-grams\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEolxrEwVdc6"},"source":["#set the seed for the random number generator (to ensure consistent results)\n","np.random.seed(123)\n","\n","#generate five sentences for the n=5 model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnalYBNMb3Jz"},"source":["As you can see, the sentences generated by the `n = 5` model seem much more natural and understandable than the sentences generated by the `n = 3` model. Nevertheless, it is important to remember that these sentences are being generated statistically -- our Python program does not actually ***understand*** the meaning of the sentences that it is writing!\n","\n","##End of Lab Assignment 03!"]}]}